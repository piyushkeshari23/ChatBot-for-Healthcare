{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QXX2Rs3S-lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5938710-3816-44e1-bff8-685d4224dcb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4fC20DARgs8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/data/training.1600000.processed.noemoticon.csv\",\n",
        "                names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                encoding=''ISO-8859-1'')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTuVwsi-HPzy"
      },
      "source": [
        "df = df.drop(columns=['id', 'date', 'query', 'user'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDI3Ur43HR4A"
      },
      "source": [
        "df = df.sample(n=100000)\n",
        "df.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEZP6MpWHt0n"
      },
      "source": [
        "df.to_csv(\"sentiment140-subset.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVAHyG2QThP8"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import sys\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation,stem_text\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\n",
        "import time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGgkwN1Hcsh"
      },
      "source": [
        "df = pd.read_csv(\"../content/sentiment140-subset.csv\",encoding='ISO-8859-1')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ESGpiVjHhM6"
      },
      "source": [
        "df.polarity = df.polarity.replace({0: 0, 4: 1})\n",
        "df.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xjv42y2Hk0r"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hc-OF-kTeDp"
      },
      "source": [
        "def export(type_data='train'):\n",
        "    print (\"Extracting data...\")\n",
        "    if type_data.lower() == 'train':\n",
        "        filename = 'sentiment140-subset.csv'\n",
        "    elif type_data.lower() == 'test':\n",
        "        filename = 'testdata.manual.2009.06.14.csv'\n",
        "    data_file = codecs.open('../content/' + filename, encoding='ISO-8859-1')\n",
        "    data = []\n",
        "    for tweet in data_file.read().split('\\n')[:-1]:\n",
        "        data.append([string for string in tweet.split('\"') if string not in ['', ',']])\n",
        "    data_file.close()\n",
        "    labels = df[\"polarity\"].tolist()\n",
        "    tweets = [tweet[-1] for tweet in data]\n",
        "    print (\"Preprocessing data...\")\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        new_tweet = ' '.join([word for word in tweet.split(' ') if len(word)> 0 and word[0] not in ['@', '#'] and 'http' not in word]).strip()\n",
        "        pro_tweet = [word[:-3] if word[-3:] == 'xxx' else word for word in preprocess_string(new_tweet.replace('not', 'notxxx'))]\n",
        "        if len(pro_tweet) < 2:\n",
        "            tweets[i] = strip_punctuation(stem_text(new_tweet.lower())).strip().split()\n",
        "        else:\n",
        "            tweets[i] = pro_tweet\n",
        "        print (\"\\r%d tweet(s) pre-processed out of %d\\r\" % (i + 1, len(tweets)))\n",
        "\n",
        "    print (\"\\nCleaning data...\")\n",
        "    backup_tweets = np.array(tweets)\n",
        "    backup_labels = np.array(labels)\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    for i, tweet in enumerate(backup_tweets):\n",
        "        if len(tweet) >= 2:\n",
        "            tweets.append(tweet)\n",
        "            labels.append(backup_labels[i-1])\n",
        "    del backup_tweets\n",
        "    del backup_labels\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    data = list(zip(tweets, labels))\n",
        "    np.random.shuffle(data)\n",
        "    tweets, labels = list(zip(*data))\n",
        "\n",
        "    return (tweets, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZ_jwEhTpQY"
      },
      "source": [
        "def create_vocab(tweets):\n",
        "    print (\"Building vocabulary...\")\n",
        "    vocab = Dictionary()    \n",
        "    vocab.add_documents(tweets)\n",
        "    vocab.save('vocab_sentiment')\n",
        "    return vocab\n",
        "\n",
        "def get_vocab(tweets=None):\n",
        "    if 'vocab_sentiment' in os.listdir('.'):\n",
        "        if not tweets:\n",
        "            print (\"Loading vocabulary...\")\n",
        "            vocab = Dictionary.load('vocab_sentiment')\n",
        "            print (\"Loaded vocabulary\")\n",
        "            return vocab\n",
        "        response = input('Vocabulary found. Do you want to load it? (Y/n): ')\n",
        "        if response.lower() in ['n', 'no', 'nah', 'nono', 'nahi', 'nein']:\n",
        "            if not tweets:\n",
        "                tweets, labels = export()\n",
        "                del labels\n",
        "            return create_vocab(tweets)\n",
        "        else:\n",
        "            print (\"Loading vocabulary...\")\n",
        "            vocab = Dictionary.load('vocab_sentiment')\n",
        "            print (\"Loaded vocabulary\")\n",
        "            return vocab\n",
        "    else:\n",
        "        if not tweets:\n",
        "            tweets, labels = export()\n",
        "            del labels\n",
        "        return create_vocab(tweets)\n",
        "\n",
        "def init_with_vocab(tweets=None, labels=None, vocab=None, type_data='train'):\n",
        "    if not tweets and not labels:\n",
        "        tweets, labels = export(type_data)\n",
        "    elif tweets and labels:\n",
        "        pass\n",
        "    else:\n",
        "        print (\"One of tweets or labels given, but not the other\")\n",
        "        return\n",
        "    if not vocab and type_data == 'train':\n",
        "        vocab = get_vocab(tweets)\n",
        "    elif not vocab:\n",
        "        vocab = get_vocab()\n",
        "\n",
        "    print (\"Replacing words with vocabulary numbers...\")\n",
        "    max_tweet_len = 20\n",
        "    numbered_tweets = []\n",
        "    numbered_labels = []\n",
        "    for tweet_num, (tweet, label) in enumerate(zip(tweets, labels)):\n",
        "        current_tweet = []\n",
        "\n",
        "        for word in tweet:\n",
        "            if word in vocab.token2id:\n",
        "                current_tweet.append(vocab.token2id[word] + 1)\n",
        "\n",
        "        if len(current_tweet) <= max_tweet_len:\n",
        "            current_tweet_len = len(current_tweet)\n",
        "            for i in range(max_tweet_len - current_tweet_len):\n",
        "                current_tweet.append(0)\n",
        "            numbered_tweets.append(current_tweet)\n",
        "            numbered_labels.append(label)\n",
        "\n",
        "        else:\n",
        "            while len(current_tweet) > max_tweet_len:\n",
        "                numbered_tweets.append(current_tweet[:max_tweet_len])\n",
        "                numbered_labels.append(label)\n",
        "                current_tweet = current_tweet[max_tweet_len:]\n",
        "            if len(current_tweet) > 1:\n",
        "                current_tweet_len = len(current_tweet)\n",
        "                for i in range(max_tweet_len - current_tweet_len):\n",
        "                    current_tweet.append(0)\n",
        "                numbered_tweets.append(current_tweet)\n",
        "                numbered_labels.append(label)\n",
        "\n",
        "    print (\"Replaced words with vocabulary numbers\")\n",
        "    del tweets\n",
        "    labels = np.array(numbered_labels).astype(np.float32)\n",
        "    del numbered_labels\n",
        "    return (numbered_tweets, labels, len(vocab))\n",
        "\n",
        "def create_nn(vocab_len=None, max_tweet_len=None):\n",
        "    if vocab_len == None:\n",
        "        print (\"Error: Vocabulary not initialized\")\n",
        "        return\n",
        "    if max_tweet_len == None:\n",
        "        print (\"Error: Please specify max tweet length\")\n",
        "        return\n",
        "\n",
        "    nn_model = Sequential()\n",
        "    nn_model.add(Embedding(input_dim=(vocab_len + 1), output_dim=32, mask_zero=True))\n",
        "    nn_model.add(LSTM(128))\n",
        "    nn_model.add(Dense(32, activation='sigmoid', kernel_regularizer=l2(0.05)))\n",
        "    nn_model.add(Dropout(0.3))\n",
        "    nn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    nn_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "    print (\"Created neural network model\")\n",
        "    return nn_model\n",
        "\n",
        "def get_nn(vocab_len=None, max_tweet_len=None):\n",
        "    if 'model_nn.h5' in os.listdir('.'):\n",
        "        response = input('Neural network model found. Do you want to loadit? (Y/n): ')\n",
        "        if response.lower() in ['n', 'no', 'nah', 'nono', 'nahi', 'nein']:\n",
        "            return create_nn(vocab_len, max_tweet_len)\n",
        "        else:\n",
        "            print (\"Loading model...\")\n",
        "            nn_model = load_model('model_nn.h5')\n",
        "            print (\"Loaded model\")\n",
        "            return nn_model\n",
        "    else:\n",
        "        return create_nn(vocab_len, max_tweet_len)\n",
        "\n",
        "def train_nn(tweets=None, labels=None, nn_model=None):\n",
        "    if tweets is None and labels is None:\n",
        "        tweets, labels, vocab_len = init_with_vocab()\n",
        "    elif tweets is not None and labels is not None:\n",
        "        pass\n",
        "    else:\n",
        "        print (\"One of tweets or labels given, but not the other\")\n",
        "        return\n",
        "    if not nn_model:\n",
        "        max_tweet_len = max([len(tweet) for tweet in tweets])\n",
        "        nn_model = get_nn(vocab_len, max_tweet_len)\n",
        "\n",
        "    # Callbacks (extra features)\n",
        "    tb_callback = TensorBoard(log_dir='./Tensorboard/' + str(time.time()))\n",
        "    early_stop = EarlyStopping(monitor='loss', min_delta=0.025, patience=6)\n",
        "    lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.5, min_lr=0.00001,patience=2, epsilon=0.1)\n",
        "    saver = ModelCheckpoint('model_nn.h5', monitor='val_acc')\n",
        "\n",
        "    try:\n",
        "        nn_model.fit(np.array(tweets),np.array(labels), epochs=35, batch_size=8192, callbacks=[tb_callback, early_stop, lr_reducer, saver], validation_split=0.2)\t\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    nn_model.save('model_nn.h5')\n",
        "    print(\"Saved model\")\n",
        "    del tweets\n",
        "    del labels\n",
        "    tweets_test, labels_test, _ = init_with_vocab(type_data='test')\n",
        "    print (nn_model.evaluate(tweets_test, labels_test, batch_size=32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvsBNnhTI66Z"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  train_nn()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}